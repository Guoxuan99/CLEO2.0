# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QuiDdYRriNJoRFvXtEgmmyTmMtxXtdkI
"""

import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
import json
import pickle
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout
from keras.optimizers import SGD
import random
from tkinter import *
import time
from colour import Color
from PIL import ImageTk, Image
import os
from keras.models import load_model
import textblob
import webbrowser
import re

# nltk.download('punkt')
# nltk.download('wordnet')
lemmatizer = WordNetLemmatizer()

from nltk.corpus import stopwords
from nltk import RegexpTokenizer

tokenizer = RegexpTokenizer(r"\w+")
# nltk.download('stopwords')

words = []
tags = []
documents = []
dataset_file = open("nlp_dataset.json").read()
j_son = json.loads(dataset_file)

for intent in j_son['intents']:
    for question in intent["questions"]:
        w = tokenizer.tokenize(question)
        words.extend(w)
        documents.append((w, intent['tag']))
        if intent['tag'] not in tags:
            tags.append(intent['tag'])

stop_words = set(stopwords.words("english"))
words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in stop_words]
words = sorted(set(words))
tags = sorted(tags)
# pickle.dump(words, open('words.pkl', 'wb'))
# pickle.dump(tags, open('tags.pkl', 'wb'))

training_set = []
empty_output = [0] * len(tags)
for doc in documents:
    bag = []
    sentence = doc[0]
    pattern = [lemmatizer.lemmatize(w.lower()) for w in sentence if w]
    bag = [1 if w in pattern else 0 for w in words]
    output = list(empty_output)
    output[tags.index(doc[1])] = 1

    training_set.append([bag, output])

random.shuffle(training_set)
training_set = np.array(training_set)
train_x = list(training_set[:, 0])
train_y = list(training_set[:, 1])

model = Sequential()
model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(len(train_y[0]), activation='softmax'))

# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model
sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])

# fitting and saving the model
hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)
# model.save('chatbot_model.h5', hist)

model = load_model('chatbot_model.h5')
intents = json.loads(open('nlp_dataset.json').read())
words = pickle.load(open('words.pkl', 'rb'))
tags = pickle.load(open('tags.pkl', 'rb'))



def clean_up_sentence(sentence):
    sentence_words = nltk.word_tokenize(sentence)
    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]
    return sentence_words


# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence

def bow(sentence, words, show_details=True):
    # tokenize the pattern
    sentence_words = clean_up_sentence(sentence)
    # bag of words - matrix of N words, vocabulary matrix
    bag = [0] * len(words)
    for s in sentence_words:
        for i, w in enumerate(words):
            if w == s:
                # assign 1 if current word is in the vocabulary position
                bag[i] = 1
                if show_details:
                    print("found in bag: %s" % w)
    return (np.array(bag))


def predict_class(sentence, model):
    # filter out predictions below a threshold
    p = bow(sentence, words, show_details=False)
    res = model.predict(np.array([p]))[0]
    ERROR_THRESHOLD = 0.25
    results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]
    # sort by strength of probability
    results.sort(key=lambda x: x[1], reverse=True)
    return_list = []
    for r in results:
        return_list.append({"intent": tags[r[0]], "probability": str(r[1])})
    if return_list == []:
        return_list.append({"intent": "default"})
    return return_list


def getResponse(msg, ints, intents_json):
    tag = ints[0]['intent']
    if tag == 'default':
        return "Sorry I do not understand."
    list_of_intents = intents_json['intents']
    for i in list_of_intents:
        if (i['tag'] == tag):
            result = i['response']
            if (tag == 'recommendation'):
                return give_recommendation(msg)
            break
    return result


def chatbot_response(msg):
    ints = predict_class(msg, model)
    res = getResponse(msg, ints, intents)
    return res


def give_recommendation(text: str):
    checkList = ['CLEO','store','shop','suggest','suggestion','recommensation','recommend']
    blob = textblob.TextBlob(text)
    noun_phrase = blob.noun_phrases
    if not noun_phrase:
        sentence = nltk.word_tokenize(text)
        list = [nouns[0] for nouns in nltk.pos_tag(sentence) if (nouns[1] == 'NN') | (nouns[1] == 'NNS')]
        for word in list:
            if word in checkList:
                continue
            url = f"https://www.lazada.com.my/catalog/?q={word}%20&rating=5"
            webbrowser.open(url=url, new=2)
            return f"Here's a recommendation on {word}.If the browser is not prompted, you can open it by using this url: {url}"
    if len(noun_phrase) == 1:
        phrase = str(noun_phrase[0]).replace(" ", "+")
        url = f"https://www.lazada.com.my/catalog/?q={phrase}%20&rating=5"
        webbrowser.open(url=url, new=2)
        return f"Here's a recommendation on {noun_phrase[0]}.If the browser is not prompted, you can open it by using this url: {url}"
    if len(noun_phrase) > 1:
        for phrase in noun_phrase:
            for i in phrase:
                if i in checkList:
                    continue
            phrase = str(noun_phrase[0]).replace(" ", "+")
            url = f"https://www.lazada.com.my/catalog/?q={phrase}%20&rating=5"
            webbrowser.open(url=url, new=2)
            return f"Here's a recommendation on {noun_phrase[0]}.If the browser is not prompted, you can open it by using this url: {url}"
    return "no recommendation provided"


def GUI():
    root = Tk()
    root.title("Customer Lazada Engagement Officer v2.0")
    root.iconbitmap(os.getcwd() + '/chatbot.ico')
    root.resizable(0, 0)

    def process(event=None):
        send = e.get()
        blob = textblob.TextBlob(send)
        send = str(blob.correct())
        txt.insert(END, "You:\n" + send + "\n\n")
        res = chatbot_response(send)
        txt.insert(END, "Bot:\n" + res + "\n\n")
        e.delete(0, END)

    my_img = ImageTk.PhotoImage(file="bgchatbot.png")
    my_label = Label(root, image=my_img)
    my_label.grid(row=0, column=2, columnspan=2, rowspan=2, sticky="NS")

    txt = Text(root, height=28, width=40, font='gotham', wrap=WORD)
    greeting = "CLEO" + "\n" + "Good day, I am CLEO, a 24/7 online chatbot that answers your queries. How can I help you?"
    txt.insert(INSERT, greeting + "\n\n")
    e = Entry(root, width=60)
    e.insert(0, "Enter your message here")

    send = Button(root, text="Send", command=process).grid(row=1, column=1)
    txt.grid(row=0, column=0, columnspan=2, sticky="NSEW")
    # txt.grid(row=0, column=0, columnspan=2, padx=(275,10))
    e.bind('<Return>', process)
    # e.grid(row=1, column =0, padx=(275,10))
    e.grid(row=1, column=0)
    # ===Single image===#
    root.mainloop()


GUI()
# def run():
#     while True:
#         text = input("What do you want?")
#         res = chatbot_response(text)
#         print(res)

# run()
